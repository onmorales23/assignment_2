---
title: "Data Analysis 2"
output: 
  pdf_document
author: Ziying Li, Matt Capaldi, Yujuan Gao, Olivia Morales
date: \today
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=T, include=T, message=F, warning=F, error=F}

## libraries
libs <- c("tidyverse", "haven", "bibtex", 
          "psych", "knitr", "pastecs", "kableExtra","survey", 
          "cobalt", "randomForest", "ipred","rpart", "baguette", 
          "parsnip", "SimDesign", "bartCause", "lme4", "grf", 
          "GenericML", "car", "bartMachine", "gtools", "patchwork")

sapply(libs, require, character.only = TRUE)

covariateNames <- c(
    "X1RTHETK1", 
    "X1MTHETK1",
    "X1TCHAPP", 
    "X1TCHCON",
    "X1TCHPER", 
    "X1TCHEXT",
    "X1TCHINT",
    "X1ATTNFS",
    "X1INBCNT",
    "X12MOMAR",
    "X1NUMSIB", 
    "P1OLDMOM",
    "P1CHLDBK",
    "P2DISTHM",
    "P1NUMPLA",
    "T2PARIN",
    "X12PAR1ED_I",
    "X12PAR2ED_I",
    "X2INCCAT_I",
    "X1PAR1EMP",
    "S2LUNCH",
    "X2KRCETH",
    "S2NGHBOR",
    "S2OUTSID",
    "S2USDABR",
    "S2PUBSOC",
    "X1LOCALE",
    "S1_ID",
    "W1_2P0PSU",
    "prop.missing",
    "X_CHSEX_R", 
    "X_RACETH_R", 
    "P1HSCALE")

## directory path (assignment_2 as current working directory)

data_dir <- file.path(".", "data")

## loading data
load(file.path(data_dir, "chapter_10_data_cleaned_and_imputed.Rdata"))

#standardized continuous predictors
for (var in covariateNames) {
  if (class(data[,var])!="factor") { data[,var] = (data[,var]-mean(data[,var]))/sd(data[,var]) } }

```

In this paper, we explore heterogeneity of treatment using three different methods; CausalBART, GenericML, and CausalForest. We will explore the heterogeneity (Questions 1, 3, and 4) separately for each model before concluding and comparing CATEs (Question 2) at the conclusion of the paper.

```{r pre q PSM model, echo = T, include = T, message=F, warning = F, error = F}


psFormula <- paste(covariateNames, collapse="+")
psFormula <- formula(paste("treated~", psFormula, sep=""))
print(psFormula) 
    
```

# BART

```{r q_1,BART, echo = T, include = T, message= F, warning = F, error = F}

# credit to Matt, esp for the covs matrices!
train <- data %>% 
  sample_frac(size = 0.5)
test <- anti_join(data, train)

train <- train %>% mutate(across(.fns = as.numeric))
test <- test %>% mutate(across(.fns = as.numeric))
train$treated <- train$treated - 1
test$treated <- test$treated - 1

#matrix_covs <- as.matrix(train %>% select(all_of(covariateNames)) %>%
                          # mutate(across(.fns = as.numeric)))


# estimating conditional average treatment effects (CATEs) using BART

bart <- bartc(response = train$X2MTHETK1,
             treatment = train$treated,
             confounders = data.frame(train[,covariateNames]),
             method.rsp = "bart", 
             method.trt = "glm",
             keepTrees = TRUE,
             estimand = "ate")

cate <- predict(bart,
               newdata = data.frame(test[,covariateNames]),
               type = "icate")

cate_m <- apply(cate, 2, mean) 
  
test$cate <- apply(cate, 2, mean) 

modely <- summary(bart, target = "cate")

```

We first attempted estimating the conditional average treatment effects (CATEs) 
of early childhood care on mathematics scores using Bayesian Additive Regression Trees (BART) for the entire dataset and for all the covariates estimated in the original propensity score model. We found that splitting the data into training and testing data mitigated severe overfitting issues (particularly in calculating the propensity scores).

note: the bartc function does not allow the typical adjustments to hyperparamters characteristic of the other functions like genericML and causal_forest, so we decided to focus just on making adjustments to the arguments of the functions as needed.

We also tried estimating the propensity scores & CATEs both with BART, but found that logistic regression was better suited in estimating the scores while still using BART for the CATEs.

The final model (modely) provides the CATE derived from the fit BART model, demonstrating no substantive treatment effect heterogeneity.


```{r q_3,BART, echo = T, include = T, message= F, warning = F, error = F}

#library(bartMachine) already in libs list 


bart_machine <- bartMachine(X=data.frame(train[,covariateNames]), 
                            y=cate_m,
                            serialize = T, 
                            mem_cache_for_speed = F,
                            seed = 20221026)


var.importance <- investigate_var_importance(bart_machine, 
                                            num_replicates_for_avg = 20,
                                            plot = F)

#select variables whose importance is greater than the median
important.vars = names(var.importance$avg_var_props)[
  var.importance$avg_var_props > median(var.importance$avg_var_props)]

bart2 <- bartc(response = test$X2MTHETK1,
              treatment = test$treated,
              confounders = data.frame(test[,important.vars]),
              method.rsp = "bart",
              method.trt = "glm",
              estimand = "ate",
              keepTrees = T) 

cate2 <- predict(bart2,
                newdata = data.frame(test[,important.vars]),
                type = "icate")

test$cate2 <- apply(cate2, 2, mean)

modely2 <- summary(bart2, target = "cate")

model <- paste(important.vars, collapse="+")
model <- paste(c("cate2",model), collapse="~")
model.cate <- lm(model,data = test)
summary(model.cate)

```

We then identified the most important variables (selecting those variables with inclusion proportions greater than the median proportion for the BART model). Those variables were then used to fit another BART object and predict CATEs once again. The model CATE was slightly greater than the first estimation with all the variables included, but not by much.

note: we reduced the num_replicates_for_avg due to questions of efficiency; we decided to decrease from 25 to 20. We also noticed the top variables in the proportion figure kept changing, even when setting a seed. As such, we identified the two predictors that occurred the most in multiple runs of the investigate_var_importance function. Results are identified below:

```{r comparing BART, echo = T, include = T, message= F, warning = F, error = F}

blabels = c("Full Model", "Important Variables Only")

# correlation matrix

cor(test$cate, test$cate2)

#boxplot
plot1 <- boxplot(test$cate, test$cate2, names = blabels, ylab = "CATEs")

# qqplot
qplot <- qqplot(test$cate, test$cate2, xlab = "Full Model", ylab = "Important Vars")

```

Especially from the boxplot, it seems that the CATEs of the important variables model were slightly enhanced but not substantively. From the correlation matrix, there is a very weak correlation noted (~0.38).

# Two most important CATE predictors (BART), plot CATE/predictor relationship

```{r q_4,BART, echo = T, include = T, message= F, warning = F, error = F}

## top 2 predictors
top.pred <- c("T2PARIN", "X1NUMSIB")

cut <- quantcut(test$X1NUMSIB, q = 4, na.rm = T)
  
## plot relationship

bplot1 <- ggplot(test, aes(x = as.factor(T2PARIN), y = cate2)) +
  geom_boxplot() + labs(y = "CATEs", x = "T2PARIN") 

bplot2 <- ggplot(test, aes(x = cut, y = cate2)) +
  geom_boxplot() + labs(y = "CATEs", x = "X1NUMSIB") +
  scale_x_discrete(guide = guide_axis(n.dodge = 3))

library(patchwork)
bplot1 + bplot2
```

From the previous estimations of variable importance, we found that T2PARIN and X1NUMSIB were the two most significant predictors (according to their relative inclusion proportion in the BART model). The two plots demonstrate a similar lack of heterogeneity, as there is no identifiable relationship between the estimates and the two predictors.

note: the X1NUMSIB variable was previously standardized, that's why the plots are divided into not the most helpful quartiles.

\newpage

# GenericML

### Q1)
Initially, we set up the hyper-parameters for GenericML. We decided to use the learners ranger (with 300 trees, for a balance of efficiency and effectiveness), and lasso with the default settings as a comparison.

As GenericML is set up to be used on experimental data and therefore does not accept propensity scores outside of 0.05 and 0.95, propensity scores were calculated and then rounded at the extreme ends to avoid this issue.

The other primary hyperparameter we were concerned with was the num_splits argument in the GenericML function (how many times the data is split for recalculating), which was set to 10. This again was  chosen for a balance efficiency and effectiveness.

```{r q_1 genML, echo = T, include = T, message = F, warning = F, error = F}

# Setup for GML
learners <- c("mlr3::lrn('ranger', num.trees = 300)", "lasso")
matrix_covs <- as.matrix(data %>% select(all_of(covariateNames)) %>%
                           mutate(across(.fns = as.numeric))) 
X1 <- setup_X1(funs_Z = c("B", "S"))
vcov <- setup_vcov(estimator = "vcovHC")

# Estimate ps scores (with 0.05/0.95 adjustment to work with GenML)
library(parsnip)
ps_rf <- rand_forest(mode = "classification",
               engine = "ranger",
               trees = 1000) %>%
  fit(psFormula,
      data = data)
data$ps_rf <- predict(ps_rf,
                      new_data = data,
                      type = "prob")[,2]
data$ps_rf <- data$ps_rf$.pred_1 ## remove the $column.name
data <- data %>%
  mutate(ps_rf = ifelse(ps_rf >= 0.95, 0.94, ps_rf), #Rounding to avoid error Dr. L
         ps_rf = ifelse(ps_rf <= 0.05, 0.06, ps_rf))

# Run initial GenML
genML <- GenericML(
  Z = matrix_covs, #covariates
  D = as.numeric(as.character(data$treated)), #treatment
  Y = as.numeric(data$X2MTHETK1), #outcome
  learners_GenericML = learners,  # learners specified above
  learner_propensity_score = as.numeric(data$ps_rf), #as.numeric(data$ps)  #ps
  num_splits = 10,                        # number splits of the data
  quantile_cutoffs = c(0.2, 0.4, 0.6, 0.8), # grouping for CATEs
  significance_level = 0.05,                # significance level
  X1_BLP = X1, X1_GATES = X1,               # regression setup
  vcov_BLP = vcov, vcov_GATES = vcov,       # covariance setup
  parallel = F, #num_cores = 6L, # parallelization
  seed = 20220621)                         # RNG seed
```

As the below printouts show, the best ranger performed more effectively than lasso, and therefore, the results calculated with ranger become the default for the rest of the analysis
```{r}
get_best(genML)
## ranger is best, becomes the default for all future GenML functions
```

Unlike when initially calculating heterogeneity on attendance, GenML does not find evidence of significant heterogeneity when looking at spring math scores. For the purpose of the assignment however, we continue analyzing as if there was heterogeneity to be explored.

```{r}
get_BLP(genML)
## (no longer) significant indicating treatment heterogeneity
```

```{r}
get_GATES(genML)
```

The below code predicts CATEs with all covariates for use in Q2 at the end of the paper.

```{r}
# Predict CATEs with full dataset for plotting/comparison
genML_Q2 <- proxy_CATE(Z = matrix_covs,
                    D = as.numeric(as.character(data$treated)),
                    Y = as.numeric(data$X2MTHETK1),
                    A_set = sample(1:12684, size = 12684/2), #obs sample half
                    learner = "mlr3::lrn('ranger', num.trees = 300)")
data$GenML_CATEa <- genML_Q2$estimates$CATE
# proxy_CATE builds model with half but then provides estimates for all, so need to 
# cut down to half sample to compare with other methods later. No better method
# appears to be available
GenML_CATEhalf <- sample(data$GenML_CATEa, 6342)

```

### Q3)
The next stage for GenericML took a workaround to best imitate the variable importance or BartMachine functions of the other two methods. Using heterogeneity_CLAN() we were able to get the p-values for each variable in terms of covariates in terms of their influence on heterogeneity.

```{r}
genML_het <- heterogeneity_CLAN(genML)

genML_sig <- as.data.frame(genML_het$p_values) %>%
  pivot_longer(cols = everything()) %>%
  arrange(value)

# Selecting variables above median significance
genML_imps <- genML_sig %>%
  filter(value < median(value)) %>%
  select(name) %>%
  as.list()
genML_imps <- genML_imps[["name"]]
```

Then, a new covariate matrix was created with only variables whose p-value was less than the median of all. As there is no directly comparable function to variable importance in causal forests and bart machine for causal bart, this was our best approximation of a similar concept.

We then recalculated CATEs with only these reduced covariates.

```{r}
# Create reduced covariates matrix
matrix_covsGML2 <- as.matrix(data %>% select(all_of(genML_imps)) %>%
                           mutate(across(.fns = as.numeric))) 

# Calculate CATES using reduced covaraites
genML_Q3 <- proxy_CATE(Z = matrix_covsGML2,
                    D = as.numeric(as.character(data$treated)),
                    Y = as.numeric(data$X2MTHETK1),
                    A_set = sample(1:12684, size = 12684/2), #obs sample half
                    learner = "mlr3::lrn('ranger', num.trees = 300)")
data$GenML_CATEr <- genML_Q3$estimates$CATE
```

The resulting CATEs are summarized below. These results indicate that there is some variation, however, as noted above the omnibus test failed to find significant heterogeneity, so cannot assume this variation is anything other than noise. The correlation between the reduced and all variable CATE is 0.49 indicating a moderate relationship as demonstrated by the plot provided below as well.

```{r}
summary(data$GenML_CATEr)

cor(data$GenML_CATEa, data$GenML_CATEr)
qqplot(data$GenML_CATEa, data$GenML_CATEr,
     main = "Comparing CATEs from GenML",
     ylab = "CATE with imp vars only",
     xlab = "CATE with all vars")
```

### Q4)

Lastly, we draw back on the calculation of individual predictors of treatment and pull out the 2 with the lowest p-values, X1ATTNFS (focus scale) and X1TCHAPP (teacher approaches to learning). Below are the plots of these variable against the CATE.
```{r}
genML_sig %>% head(n = 2)
# Show the 2 lowest p values of het, i.e. most significant predictors of het

a <- ggplot(data) +
  geom_point(aes(x = X1ATTNFS,
                 y = GenML_CATEa),
             alpha = 0.8) +
  labs(y = "CATE",
       x = "Focus Scale")


b <- ggplot(data) +
  geom_point(aes(x = X1TCHAPP,
                 y = GenML_CATEa),
             alpha = 0.8) +
  labs(y = "CATE",
       x = "Teacher Approach")

library(patchwork)
a + b

```

Even though these variables were identified as the mostly likely contributors to treatment heterogeneity, the 2 plots show quite there is no clear relationship to be seen between them and the CATE, supporting the finding that GenericML suggests there is no treatment heterogeneity.

\newpage

# Causal Forests

```{r q_1 CF, echo = T, include = T, message = F, warning = F, error = F}

data2 <- data

#fit logistic regression model for propensity score estimation ignoring clustering
ps.model0 <- glm(psFormula, data=data2, family=binomial)

#obtain propensity scores that ignore clustering
data2$ps <- fitted(ps.model0)

#the grf package only takes numeric covariates
#So convert those factor variables to be the numeric class
for (i in 1:length(covariateNames)) {
  if(class(data2[,covariateNames[i]])=="factor"){
    data2[, covariateNames[i]] <- as.numeric(as.character(data2[,covariateNames[i]]))
  }
}

#Step 1: Split data into training data set and testing data set
#In this case, we split it to be 50/50
set.seed(123)
train_index <- sample(1:nrow(data2), nrow(data2)/2)
train_index <- train_index[order(train_index)]

train_data <- data2[train_index,]
test_data <- data2[-train_index,]

#Step 2: model fit, using causal forest
#Tuning mtry and min.node.size parameters by setting tune.parameters
train.forest = causal_forest(X=train_data[,covariateNames],
                              Y = train_data$X2MTHETK1, num.trees = 5000,
                              W = as.numeric(as.character(train_data$treated)),
                              W.hat = train_data$ps,
                              tune.parameters = c("mtry", "min.node.size"),
                              seed = 0)

#The best tunning parameters of mtry and min.node.size were shown below, which indicated a better performance than the default setting
train.forest[["tuning.output"]]



```

# Figures Comparing Different CF fits

```{r q_2 CF, echo = T, include = T, message = F, warning = F, error = F}
#Obtain estimates of the conditional average treatment effect (CATE)
#with standard errors
tau.hat = predict(train.forest,X= test_data[,covariateNames], estimate.variance = T)
CATE_causalForest = tau.hat$predictions


# Causal Forests
#1. correlation matrix
#causal forest only output the best tunning parameters' model fit outcomes
#To answer Q2, I run one more model fit with mtry = 4 and min.node.size = 50
train.forest2 = causal_forest(X=train_data[,covariateNames],
                              Y = train_data$X2MTHETK1, num.trees = 5000,
                              W = as.numeric(as.character(train_data$treated)),
                              W.hat = train_data$ps,
                              mtry = 4, min.node.size = 50,
                              seed = 0)
tau.hat2 = predict(train.forest2,X= test_data[,covariateNames], estimate.variance = T)
CATE2_causalForest = tau.hat2$predictions

cor(CATE_causalForest, CATE2_causalForest)

#2.box plot
boxplot(CATE_causalForest, CATE2_causalForest)

#3.QQ plot
qqplot(CATE_causalForest, CATE2_causalForest)

#The correlation between the two tunning methods was strong. But the distributions of CATEs between these two tunning methods were slightly different (see box plot), which caused a nonlinear QQ plot.
```

# Determine Best Linear Projection of CATE/Variable Importance (Causal Forests)

```{r q_3, echo = T, include = T, message = F, warning = F, error = F}

#Causal Forests
#Step 1: Subset important variables
importance_cf = variable_importance(train.forest)
rownames(importance_cf) = names(train_data[,covariateNames])

#select variables above the median of importance of the aggregated importances
#across imputed datasets
important.var_cf = rownames(importance_cf)[importance_cf>median(importance_cf)]

#Step 2
#run test forest with the best hyperparameters
test.forest = causal_forest(X = test_data[,important.var_cf],
                            Y = test_data$X2MTHETK1,
                            W = as.numeric(as.character(test_data$treated)),
                            W.hat = test_data$ps,
                            mtry = 22, num.trees=5000,
                            min.node.size = 1, seed = 0)

#Step 3: Estimate the best linear projection of a conditional average treatment effect using a causal forest
predictors = test_data[,important.var_cf]

CATE.prediction = best_linear_projection(test.forest, A=predictors)
CATE.prediction

#The results showed no significant differences in each covariate (p > 0.05).
#check the herterogeneity
test_calibration(test.forest)
#The outcomes showed that the coefficient of the mean forest prediction was 1 
 #which indicated the mean forest prediction was correct. 
#Also, the results indicated no heterogeneity been detected.


```

# Two most important CATE predictors (Causal Forests), plot CATE/predictor relationship


```{r q_4, echo = T, include = T, message = F, warning = F, error = F}

#The most two predictors are
Top2predictors <- c("X1TCHEXT", "P1OLDMOM")

#Obtain estimates of the conditional average treatment effect (CATE)
#with standard errors
tau.hat = predict(test.forest,X= test_data[,important.var_cf], estimate.variance = T)
CATE_causalForest = tau.hat$predictions


#Predictor 1
group1 <- quantile(train_data[, Top2predictors[1]])
train_data$groupX1M <- ifelse(train_data[,Top2predictors[1]] >= group1[4], 4, 
                             ifelse(train_data[,Top2predictors[1]]>=group1[3] & train_data[,Top2predictors[1]]<group1[4], 3,
                                    ifelse(train_data[,Top2predictors[1]]>=group1[2] & train_data[,Top2predictors[1]]<group1[3], 2,
                                           1)))
train_data$groupX1M <- factor(train_data$groupX1M)
boxplot(CATE_causalForest ~ train_data$groupX1M, xlab = paste(Top2predictors[1], "Group"), ylab = "CATE")

#Predictor 2
group2 <- quantile(train_data[, Top2predictors[2]])
train_data$group2 <- ifelse(train_data[,Top2predictors[2]] >= group2[4], 4, 
                             ifelse(train_data[,Top2predictors[2]]>=group2[3] & train_data[,Top2predictors[2]]<group2[4], 3,
                                    ifelse(train_data[,Top2predictors[2]]>=group2[2] & train_data[,Top2predictors[2]]<group2[3], 2,
                                           1)))
train_data$group2 <- factor(train_data$group2)
boxplot(CATE_causalForest ~ train_data$group2, xlab = paste(Top2predictors[2], "Group"), ylab = "CATE")

#Both two plots for two important predictors did not show an obvious different CATEs among groups.

```

\newpage

# Comparisons across Methods (Q2)
```{r  q2 supplementary, total, echo = T, include = T, message= F, warning = F, error = F}

labels = c("BART", "Causal Forests", "GenericML")

# correlation matrix

cor(cate_m, CATE_causalForest)
cor(cate_m, GenML_CATEhalf)
cor(CATE_causalForest, GenML_CATEhalf)

# qqplot 

qqplot(cate_m, CATE_causalForest)
qqplot(cate_m, GenML_CATEhalf)
qqplot(CATE_causalForest, GenML_CATEhalf)

# boxplot 

boxplot(cate_m, CATE_causalForest, GenML_CATEhalf,
                        names = labels, ylab = "CATE")
  
```

# Method Comparisons and Conclusion

BART detected no treatment effect heterogeneity, as evidenced by the CATE parameter estimate of almost 0. The CATE estimates between the full model with all covariates and the important variables model were slightly correlated. The two most important predictors identified were not related to our CATE estimates, further evidence of lack of treatment heterogeneity.


GenericML failed to find any evidence of treatment heterogeneity on the math score outcome variable. The p-value of the omnibus test for heterogeneity was 0.41, so not even close to marginal significance. The CATEs calculated with all vars and only important vars were loosely similar to each other. The two most important variables still showed no real evidence of treatment heterogeneity.

Causal Forest (CF) did not fit very well with the data set. Because when using the training data set, CF with the best tuning parameters detected heterogeneity, but it failed to detect heterogeneity by using the testing data set. When plotting the top two important predictors with CATE, we found no difference between groups.

Comparing CATEs estimated between methods we found there was surprisingly little correlation between any of the methods. Although they all predicted a CATE average close to 0, BART and Causal Forest had much narrower spreads than GenericML. This may have been in part due to the more awkward nature of pulling the CATE out GenericML, as it is not available from the primary GenericML object and needs a separate calculation.

In summary, this paper attempted to explore treatment effect heterogeneity with Causal Forests, GenericML, and Causal BART. Unfortunately, there was no significant heterogeneity to be found in terms of math score outcome. All three methods came to similar conclusions.

```{r Code Junkyard, eval=FALSE}
