---
title: "Data Analysis 2"
output: pdf_document
author: Ziying Li, Matt Capaldi, Yujuan Gao, Olivia Morales
date: \today
---

```{r setup, echo=T, include=T, message=F, warning=F, error=F}

## libraries
libs <- c("tidyverse", "haven", "bibtex", "psych", "knitr", "pastecs", "kableExtra","survey", "cobalt", "randomForest", "ipred","rpart", "baguette", "parsnip", "SimDesign", "bartCause", "lme4", "grf", "GenericML")

sapply(libs, require, character.only = TRUE)

## directory path (assignment_2 as current working directory)

data_dir <- file.path(".", "data")

## loading data
load(file.path(data_dir, "chapter_10_data_cleaned_and_imputed.Rdata"))

train <- data %>% 
  sample_frac(size = 0.5)

test <- anti_join(data, train)

train <- train %>% mutate(across(.fns = as.numeric))    
test <- test %>% mutate(across(.fns = as.numeric))    
```

# Defining/Estimating Propensity Score Model

```{r pre q PSM model, echo = T, include = T, message=F, warning = F, error = F}

covariateNames <- c(
    "X1RTHETK1", 
    "X1MTHETK1",
    "X1TCHAPP", 
    "X1TCHCON",
    "X1TCHPER", 
    "X1TCHEXT",
    "X1TCHINT",
    "X1ATTNFS",
    "X1INBCNT",
    "X12MOMAR",
    "X1NUMSIB", 
    "P1OLDMOM",
    "P1CHLDBK",
    "P2DISTHM",
    "P1NUMPLA",
    "T2PARIN",
    "X12PAR1ED_I",
    "X12PAR2ED_I",
    "X2INCCAT_I",
    "X1PAR1EMP",
    "S2LUNCH",
    "X2KRCETH",
    "S2NGHBOR",
    "S2OUTSID",
    "S2USDABR",
    "S2PUBSOC",
    "X1LOCALE",
    "S1_ID",
    "W1_2P0PSU",
    "prop.missing")

covariateNamesBART <- c(
    "X1RTHETK1", 
    "X1MTHETK1",
    "X1TCHAPP", 
    "X1TCHCON",
    "X1TCHPER", 
    "X1TCHEXT",
    "X1TCHINT",
    "X1ATTNFS",
    "X1INBCNT",
    "X12MOMAR",
    "X1NUMSIB", 
    "P1OLDMOM",
    "P1CHLDBK",
    "P2DISTHM",
    "P1NUMPLA",
    "T2PARIN",
    "X12PAR1ED_I",
    "X12PAR2ED_I",
    "X2INCCAT_I",
    "X1PAR1EMP",
    "S2LUNCH",
    "X2KRCETH",
    "S2NGHBOR",
    "S2OUTSID",
    "S2USDABR",
    "S2PUBSOC",
    "X1LOCALE")
    
```

# Estimating CATEs Using Machine Learning Methods
## BART
```{r q_1,BART, echo = T, include = T, message= F, warning = F, error = F}

# estimating conditional average treatment effects (CATEs) using BART

bart <- bartc(response = train$T2TTABS,
             treatment = train$treated,
             confounders = data.frame(train[,covariateNamesBART]),
             method.rsp = "bart", 
             method.trt = "bart",
             keepTrees = TRUE,
             estimand = "ate")

cate <- predict(bart,
               newdata = data.frame(test[,covariateNamesBART]),
               type = "icate")

cate_m <- apply(cate, 2, mean) 

```

## GenericML 
```{r q_1 genML, echo = T, include = T, message = F, warning = F, error = F}
##' @Matt

learners <- c("random_forest", "lasso")

matrix_covs <- as.matrix(data %>% select(covariateNames) %>%
                           mutate(across(.fns = as.numeric))) 

X1 <- setup_X1(funs_Z = c("B", "S"))
               #fixed_effects = vil_pair)

vcov <- setup_vcov(estimator = "vcovHC")
                   #arguments = list(cluster = demi_paire))

library(parsnip)
ps_nnet <- mlp(mode = "classification",
               engine = "nnet",
hidden_units = 20) %>%
  fit(psFormula,
      data = data)
data$ps_nnet <- predict(ps_nnet,
                      new_data = data,
                      type = "prob")[,2]
data$ps_nnet <- data$ps_nnet$.pred_1 ## remove the $column.name

genML <- GenericML(
  Z = matrix_covs, #covariates
  D = as.numeric(as.character(data$treated)), #treatment
  Y = as.numeric(data$T2TTABS), #outcome
  learners_GenericML = learners,  # learners specified above
  learner_propensity_score = as.numeric(data$ps_nnet), #as.numeric(data$ps)  #ps
  num_splits = 10,                        # number splits of the data
  quantile_cutoffs = c(0.2, 0.4, 0.6, 0.8), # grouping for CATEs
  significance_level = 0.05,                # significance level
  X1_BLP = X1, X1_GATES = X1,               # regression setup
  vcov_BLP = vcov, vcov_GATES = vcov,       # covariance setup
  parallel = F, #num_cores = 6L, # parallelization
  seed = 20220621)                         # RNG seed

get_best(genML)
## random_forest is best, becomes the default for all future GenML functions

get_BLP(genML)
## Î²2 is significant indicating treatment heterogeneity

a <- get_GATES(genML) %>%
  plot()


# Plot parental education
b <- get_CLAN(genML,
         variable = "X12PAR1ED_I") %>%
  plot() +
  labs(title = "Heterogeneity by First Parents Education",
       y = str_wrap("Average Value of Parental Education", 25))

# Plot family marriage status
c <- get_CLAN(genML,
         variable = "X12MOMAR") %>%
  plot() +
  labs(title = "Heterogeneity by Parental Marriage Status",
       y = str_wrap("Average Value of Parental Marriage Status", 25))


# Plot lunch variable
d <- get_CLAN(genML,
         variable = "S2LUNCH") %>%
  plot() +
  labs(title = "Heterogeneity by Free School Lunch",
       y = str_wrap("Average Value of Students Receiving Free Lunch", 25))

# Plot percent coming from neighborhood
e <- get_CLAN(genML,
         variable = "S2NGHBOR") %>%
  plot() +
  labs(title = "Heterogeneity by Percent of School coming from 
       Surrounding Neighborhood",
       y = str_wrap("Average Value of Percent Coming from Neighborhood", 25))


library(patchwork)
layout <- c("#AA#
            BBCC
            DDEE")
a + b + c + d + e +
  plot_layout(design = layout)

```


## causal forests
```{r q_1 CF, echo = T, include = T, message = F, warning = F, error = F}
#the grf package only takes numeric covariates
data2 <- data
#So convert those factor variables to be the numeric class
for (i in 1:length(covariateNames)) {
  if(class(data2[,covariateNames[i]])=="factor"){
    data2[, covariateNames[i]] <- as.numeric(as.character(data2[,covariateNames[i]]))
  }
}

#Step 1: Split data into training data set and testing data set
#In this case, we split it to be 50/50
set.seed(123)
train_index <- sample(1:nrow(data2), nrow(data2)/2)
train_index <- train_index[order(train_index)]

train_data <- data2[train_index,]
test_data <- data2[-train_index,]

#Step 2: model fit, using causal forest
#Tuning mtry and min.node.size parameters by setting tune.parameters
train.forest = causal_forest(X=train_data[,covariateNames],
                              Y = train_data$X2MTHETK1, num.trees = 5000,
                              W = as.numeric(as.character(train_data$treated)),
                              W.hat = train_data$ps,
                              tune.parameters = c("mtry", "min.node.size"),
                              seed = 0)

train.forest[["tuning.output"]]
#The results showed that mtry = 16 and min.node.size = 1 perform better than the default setting

#Step 3: Obtain estimates of the conditional average treatment effect (CATE)
#with standard errors
tau.hat = predict(train.forest,X= test_data[,covariateNames], estimate.variance = T)
CATE_causalForest = tau.hat$predictions


```

# Figures for Three Separate Methods

```{r q_2, echo = T, include = T, message = F, warning = F, error = F}

# BART
ggplot(data, aes(x=logit_scores, color = selfEmploy)) + 
  geom_boxplot() + ggtitle("Logit Regression")

# GenericML
ggplot(data, aes(x=forest_scores, color = selfEmploy)) + 
  geom_boxplot() + ggtitle("Random Forest")

ggplot(data, aes(x=GBM_scores, color = selfEmploy)) + 
  geom_boxplot() + ggtitle("GBM")

# Causal Forests
#1. correlation matrix
#causal forest only output the best tunning parameters' model fit outcomes
#To answer Q2, I run one more model fit with mtry = 4 and min.node.size = 50
train.forest2 = causal_forest(X=train_data[,covariateNames],
                              Y = train_data$X2MTHETK1, num.trees = 5000,
                              W = as.numeric(as.character(train_data$treated)),
                              W.hat = train_data$ps,
                              mtry = 4, min.node.size = 50,
                              seed = 0)
tau.hat2 = predict(train.forest2,X= test_data[,covariateNames], estimate.variance = T)
CATE2_causalForest = tau.hat2$predictions

cor(CATE_causalForest, CATE2_causalForest)

#2.box plot
boxplot(CATE_causalForest, CATE2_causalForest)

#3.QQ plot
qqplot(CATE_causalForest, CATE2_causalForest)


```

# Determine Best Linear Projection of CATE/Variable Importance
```{r q_3, echo = T, include = T, message = F, warning = F, error = F}
#BART


#GenericML


#Causal Forests
#Step 1: Subset important variables
importance_cf = variable_importance(train.forest)
rownames(importance_cf) = names(train_data[,covariateNames])

#select variables above the median of importance of the aggregated importances
#across imputed datasets
important.var_cf = rownames(importance_cf)[importance_cf>median(importance_cf)]

#Step 2
#run test forest
test.forest = causal_forest(X = test_data[,important.var_cf],
                            Y = test_data$X2MTHETK1,
                            W = as.numeric(as.character(test_data$treated)),
                            W.hat = test_data$ps,
                            mtry = 16, num.trees=5000,
                            min.node.size = 1, seed = 0)

#Step 3: Predict the conditional average treatment effect (CATE)
tau.hat = predict(test.forest,X= test_data[,important.var_cf], estimate.variance = T)
CATE_test = tau.hat$predictions
summary(CATE_test)

#When using the test data set and fitting the important variables,
 #the conditional ATE was small, which ranges from -0.401 to 0.254 with a mean of 0.018.

test_calibration(test.forest)
#The outcomes showed that the coefficient of the mean forest prediction was 1 
 #which indicated the mean forest prediction was correct. 
#Also, the results indicated no heterogeneity been detected.


```


# Two most important CATE predictors (For each method), plot CATE/predictor relationship
```{r q_4, echo = T, include = T, message = F, warning = F, error = F}
#BART


#GenericML


#Causal Forests
#The most two predictors are
Top2predictors <- importance_cf[order(importance_cf, decreasing = T),][1:2]

#Predictor 1
groupX1MTHETK1 <- quantile(test_data[, names(Top2predictors)[1]])
test_data$groupX1M <- ifelse(test_data$X1MTHETK1 >= groupX1MTHETK1[4], 4, 
                             ifelse(test_data$X1MTHETK1>=groupX1MTHETK1[3] & test_data$X1MTHETK1<groupX1MTHETK1[4], 3,
                                    ifelse(test_data$X1MTHETK1>=groupX1MTHETK1[2] & test_data$X1MTHETK1<groupX1MTHETK1[3], 2,
                                           1)))
test_data$groupX1M <- factor(test_data$groupX1M)
boxplot(CATE_causalForest ~ test_data$groupX1M, xlab = "X1MTHETK1 Group", ylab = "CATE")

#Predictor 2
groupX1R <- quantile(test_data[, names(Top2predictors)[2]])
test_data$groupX1R <- ifelse(test_data$X1RTHETK1 >= groupX1R[4], 4, 
                             ifelse(test_data$X1RTHETK1>=groupX1R[3] & test_data$X1RTHETK1<groupX1R[4], 3,
                                    ifelse(test_data$X1RTHETK1>=groupX1R[2] & test_data$X1RTHETK1<groupX1R[3], 2,
                                           1)))
test_data$groupX1R <- factor(test_data$groupX1R)
boxplot(CATE_causalForest ~ test_data$groupX1R, xlab = "X1RTHETK1 Group", ylab = "CATE")




```

