---
title: "Data Analysis 2"
output: 
  pdf_document
author: Ziying Li, Matt Capaldi, Yujuan Gao, Olivia Morales
date: \today
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=T, include=T, message=F, warning=F, error=F}

## libraries
libs <- c("tidyverse", "haven", "bibtex", "psych", "knitr", "pastecs", "kableExtra","survey", "cobalt", "randomForest", "ipred","rpart", "baguette", "parsnip", "SimDesign", "bartCause", "lme4", "grf", "GenericML", "car", "bartMachine", "gtools")

sapply(libs, require, character.only = TRUE)

covariateNames <- c(
    "X1RTHETK1", 
    "X1MTHETK1",
    "X1TCHAPP", 
    "X1TCHCON",
    "X1TCHPER", 
    "X1TCHEXT",
    "X1TCHINT",
    "X1ATTNFS",
    "X1INBCNT",
    "X12MOMAR",
    "X1NUMSIB", 
    "P1OLDMOM",
    "P1CHLDBK",
    "P2DISTHM",
    "P1NUMPLA",
    "T2PARIN",
    "X12PAR1ED_I",
    "X12PAR2ED_I",
    "X2INCCAT_I",
    "X1PAR1EMP",
    "S2LUNCH",
    "X2KRCETH",
    "S2NGHBOR",
    "S2OUTSID",
    "S2USDABR",
    "S2PUBSOC",
    "X1LOCALE",
    "S1_ID",
    "W1_2P0PSU",
    "prop.missing",
    "X_CHSEX_R", "X_RACETH_R", "P1HSCALE")

covariateNamesBART <- c(
    "X1RTHETK1", 
    "X1MTHETK1",
    "X1TCHAPP", 
    "X1TCHCON",
    "X1TCHPER", 
    "X1TCHEXT",
    "X1TCHINT",
    "X1ATTNFS",
    "X1INBCNT",
    "X12MOMAR",
    "X1NUMSIB", 
    "P1OLDMOM",
    "P1CHLDBK",
    "P2DISTHM",
    "P1NUMPLA",
    "T2PARIN",
    "X12PAR1ED_I",
    "X12PAR2ED_I",
    "X2INCCAT_I",
    "X1PAR1EMP",
    "S2LUNCH",
    "X2KRCETH",
    "S2NGHBOR",
    "S2OUTSID",
    "S2USDABR",
    "S2PUBSOC",
    "X1LOCALE",
    "X_CHSEX_R", "X_RACETH_R", "P1HSCALE")

## directory path (assignment_2 as current working directory)

data_dir <- file.path(".", "data")

## loading data
load(file.path(data_dir, "chapter_10_data_cleaned_and_imputed.Rdata"))

#standardized continuous predictors
for (var in covariateNames) {
  if (class(data[,var])!="factor") { data[,var] = (data[,var]-mean(data[,var]))/sd(data[,var]) } }

```

In this paper, we explore heterogeneity of treatment using three different methods; CausalBART, GenericML, and CausalForest. We will explore the heterogeneity (answer the assignment questions) separately for each model before concluding and comparing the findings at the conclusion of the paper.

```{r pre q PSM model, echo = T, include = T, message=F, warning = F, error = F}


psFormula <- paste(covariateNames, collapse="+")
psFormula <- formula(paste("treated~", psFormula, sep=""))
print(psFormula) 
    
```

# BART

```{r q_1,BART, echo = T, include = T, message= F, warning = F, error = F}

# credit to Matt, esp for the covs matrices!
train <- data %>% 
  sample_frac(size = 0.5)
test <- anti_join(data, train)

matrix_covsBART <- as.matrix(train %>% select(covariateNamesBART) %>%
                           mutate(across(.fns = as.numeric))) 

matrix_covsBART_2 <- as.matrix(test %>% select(covariateNamesBART) %>%
                           mutate(across(.fns = as.numeric))) 

# estimating conditional average treatment effects (CATEs) using BART

bart <- bartc(response = train$X2MTHETK1,
             treatment = as.numeric(as.character(train$treated)),
             confounders = matrix_covsBART,
             method.rsp = "bart", 
             method.trt = "glm",
             keepTrees = TRUE,
             estimand = "ate")

cate <- predict(bart,
               newdata = matrix_covsBART_2,
               type = "icate")

cate_m <- apply(cate, 2, mean) 


library(bartMachine)

bart_machine <- bartMachine(X=as.data.frame(matrix_covsBART), 
                            y=cate_m,
                            serialize = T, 
                            mem_cache_for_speed = F)


var.importance <- investigate_var_importance(bart_machine, 
                                            num_replicates_for_avg = 25)

#select variables whose importance is greater than the median
important.vars = names(var.importance$avg_var_props)[
  var.importance$avg_var_props > median(var.importance$avg_var_props)]

test <- test %>% mutate(across(.fns = as.numeric))
test$treated <- test$treated - 1

bart2 <- bartc(response = test$X2MTHETK1,
              treatment = test$treated,
              confounders = data.frame(test[,important.vars]),
              method.rsp = "bart",
              method.trt = "glm",
              estimand = "ate",
              keepTrees = T) 

cate2 <- predict(bart2,
                newdata = data.frame(test[,important.vars]),
                type = "icate")

test$cate2 <- apply(cate2, 2, mean)

model <- paste(important.vars, collapse="+")
model <- paste(c("cate2",model), collapse="~")
model.cate <- lm(model,data = test)
summary(model.cate)

```



```{r q_2,BART, echo = T, include = T, message= F, warning = F, error = F}

# correlation matrix

# cor(bart2, cate2, method = c("pearson", "kendall", "spearman"))

# qqplot 

# plot1 <- gglot(sample = bart2, data = chapter_10_data_cleaned_and_imputed.Rdata, color=cyl)+theme_bw()


```

# Two most important CATE predictors (BART), plot CATE/predictor relationship

```{r q_4,BART, echo = T, include = T, message= F, warning = F, error = F}

## top 2 predictors
top.pred <- c("X1TCHINT", "X1NUMSIB")

qpredvar_tch <- quantcut(test$X1TCHINT, na.rm = TRUE)
qpredvar_num <-  quantcut(test$X1NUMSIB, na.rm = TRUE)

## plot relationship

bplot1 <- ggplot(test, aes(x = qpredvar_tch, y = cate2)) +
  geom_boxplot()

bplot2 <- ggplot(test, aes(x = qpredvar_num, y = cate2)) +
  geom_boxplot()

```


# GenericML

### Q1)
```{r q_1 genML, echo = T, include = T, message = F, warning = F, error = F}
##' @Matt
learners <- c("random_forest", "lasso")
matrix_covs <- as.matrix(data %>% select(all_of(covariateNames)) %>%
                           mutate(across(.fns = as.numeric))) 
X1 <- setup_X1(funs_Z = c("B", "S"))
               #fixed_effects = vil_pair)
vcov <- setup_vcov(estimator = "vcovHC")
                   #arguments = list(cluster = demi_paire))


# Estimate ps scores (with 0.05/0.95 adjustment to work with GenML)
library(parsnip)
ps_rf <- rand_forest(mode = "classification",
               engine = "ranger",
               trees = 1000) %>%
  fit(psFormula,
      data = data)
data$ps_rf <- predict(ps_rf,
                      new_data = data,
                      type = "prob")[,2]
data$ps_rf <- data$ps_rf$.pred_1 ## remove the $column.name
data <- data %>%
  mutate(ps_rf = ifelse(ps_rf >= 0.95, 0.94, ps_rf), #Rounding to avoid error Dr. L
         ps_rf = ifelse(ps_rf <= 0.05, 0.06, ps_rf))

# Run GenML
genML <- GenericML(
  Z = matrix_covs, #covariates
  D = as.numeric(as.character(data$treated)), #treatment
  Y = as.numeric(data$X2MTHETK1), #outcome
  learners_GenericML = learners,  # learners specified above
  learner_propensity_score = as.numeric(data$ps_rf), #as.numeric(data$ps)  #ps
  num_splits = 10,                        # number splits of the data
  quantile_cutoffs = c(0.2, 0.4, 0.6, 0.8), # grouping for CATEs
  significance_level = 0.05,                # significance level
  X1_BLP = X1, X1_GATES = X1,               # regression setup
  vcov_BLP = vcov, vcov_GATES = vcov,       # covariance setup
  parallel = F, #num_cores = 6L, # parallelization
  seed = 20220621)                         # RNG seed

best <- get_best(genML)
## random_forest is best, becomes the default for all future GenML functions
base <- get_BLP(genML)
## significant indicating treatment heterogeneity


a <- get_GATES(genML) %>%
  plot()
# Plot parental education
b <- get_CLAN(genML,
         variable = "X12PAR1ED_I") %>%
  plot() +
  labs(title = "Heterogeneity by First Parents Education",
       y = str_wrap("Average Value of Parental Education", 25))
# Plot family marriage status
c <- get_CLAN(genML,
         variable = "X12MOMAR") %>%
  plot() +
  labs(title = "Heterogeneity by Parental Marriage Status",
       y = str_wrap("Average Value of Parental Marriage Status", 25))
# Plot lunch variable
d <- get_CLAN(genML,
         variable = "S2LUNCH") %>%
  plot() +
  labs(title = "Heterogeneity by Free School Lunch",
       y = str_wrap("Average Value of Students Receiving Free Lunch", 25))
# Plot percent coming from neighborhood
e <- get_CLAN(genML,
         variable = "S2NGHBOR") %>%
  plot() +
  labs(title = "Heterogeneity by Percent of School coming from 
       Surrounding Neighborhood",
       y = str_wrap("Average Value of Percent Coming from Neighborhood", 25))

a
```

\newpage

```{r, out.height = "49%"}
library(patchwork)
(b+c)/(d+e)
```

### Q3)
```{r}
genML_het <- heterogeneity_CLAN(genML)

genML_sig <- as.data.frame(genML_het$p_values) %>%
  pivot_longer(cols = everything()) %>%
  arrange(value)

#Selecting variables above median importance
genML_imps <- genML_sig %>%
  filter(value > median(value)) %>%
  select(name) %>%
  as.list()
genML_imps <- genML_imps[["name"]]

# Create reduced covariates matrix
matrix_covsGML2 <- as.matrix(data %>% select(all_of(genML_imps)) %>%
                           mutate(across(.fns = as.numeric))) 

# Calculate CATES using reduced covaraites
genML_Q3 <- proxy_CATE(Z = matrix_covsGML2,
                    D = as.numeric(as.character(data$treated)),
                    Y = as.numeric(data$X2MTHETK1),
                    A_set = sample(1:12684, size = 12684/2), #obs sample half
                    learner = "random_forest")
data$GenML_CATE2 <- genML_Q3$estimates$CATE

```



### 4)
```{r}
genML_sig %>% head(n = 2)
# Show the 2 lowest p values of het, i.e. most significant predictors of het

```






```{r}

```

\newpage

# Causal Forests

```{r q_1 CF, echo = T, include = T, message = F, warning = F, error = F}

data2 <- data

#fit logistic regression model for propensity score estimation ignoring clustering
ps.model0 <- glm(psFormula, data=data2, family=binomial)

#obtain propensity scores that ignore clustering
data2$ps <- fitted(ps.model0)

#the grf package only takes numeric covariates
#So convert those factor variables to be the numeric class
for (i in 1:length(covariateNames)) {
  if(class(data2[,covariateNames[i]])=="factor"){
    data2[, covariateNames[i]] <- as.numeric(as.character(data2[,covariateNames[i]]))
  }
}

#Step 1: Split data into training data set and testing data set
#In this case, we split it to be 50/50
set.seed(123)
train_index <- sample(1:nrow(data2), nrow(data2)/2)
train_index <- train_index[order(train_index)]

train_data <- data2[train_index,]
test_data <- data2[-train_index,]

#Step 2: model fit, using causal forest
#Tuning mtry and min.node.size parameters by setting tune.parameters
train.forest = causal_forest(X=train_data[,covariateNames],
                              Y = train_data$X2MTHETK1, num.trees = 5000,
                              W = as.numeric(as.character(train_data$treated)),
                              W.hat = train_data$ps,
                              tune.parameters = c("mtry", "min.node.size"),
                              seed = 0)

#The best tunning parameters of mtry and min.node.size were shown below, which indicated a better performance than the default setting
train.forest[["tuning.output"]]



```

# Figures Comparing Different CF fits

```{r q_2 CF, echo = T, include = T, message = F, warning = F, error = F}
#Obtain estimates of the conditional average treatment effect (CATE)
#with standard errors
tau.hat = predict(train.forest,X= test_data[,covariateNames], estimate.variance = T)
CATE_causalForest = tau.hat$predictions


# Causal Forests
#1. correlation matrix
#causal forest only output the best tunning parameters' model fit outcomes
#To answer Q2, I run one more model fit with mtry = 4 and min.node.size = 50
train.forest2 = causal_forest(X=train_data[,covariateNames],
                              Y = train_data$X2MTHETK1, num.trees = 5000,
                              W = as.numeric(as.character(train_data$treated)),
                              W.hat = train_data$ps,
                              mtry = 4, min.node.size = 50,
                              seed = 0)
tau.hat2 = predict(train.forest2,X= test_data[,covariateNames], estimate.variance = T)
CATE2_causalForest = tau.hat2$predictions

cor(CATE_causalForest, CATE2_causalForest)

#2.box plot
boxplot(CATE_causalForest, CATE2_causalForest)

#3.QQ plot
qqplot(CATE_causalForest, CATE2_causalForest)


```

# Determine Best Linear Projection of CATE/Variable Importance (Causal Forests)

```{r q_3, echo = T, include = T, message = F, warning = F, error = F}

#Causal Forests
#Step 1: Subset important variables
importance_cf = variable_importance(train.forest)
rownames(importance_cf) = names(train_data[,covariateNames])

#select variables above the median of importance of the aggregated importances
#across imputed datasets
important.var_cf = rownames(importance_cf)[importance_cf>median(importance_cf)]

#Step 2
#run test forest with the best hyperparameters
test.forest = causal_forest(X = test_data[,important.var_cf],
                            Y = test_data$X2MTHETK1,
                            W = as.numeric(as.character(test_data$treated)),
                            W.hat = test_data$ps,
                            mtry = 22, num.trees=5000,
                            min.node.size = 1, seed = 0)

#Step 3: Estimate the best linear projection of a conditional average treatment effect using a causal forest
predictors = test_data[,important.var_cf]

CATE.prediction = best_linear_projection(test.forest, A=predictors)
CATE.prediction

#The results showed no significant differences in each covariate (p > 0.05).
test_calibration(test.forest)
#The outcomes showed that the coefficient of the mean forest prediction was 1 
 #which indicated the mean forest prediction was correct. 
#Also, the results indicated no heterogeneity been detected.


```

# Two most important CATE predictors (Causal Forests), plot CATE/predictor relationship


```{r q_4, echo = T, include = T, message = F, warning = F, error = F}

#The most two predictors are
Top2predictors <- c("X1TCHEXT", "P1OLDMOM")

#Obtain estimates of the conditional average treatment effect (CATE)
#with standard errors
tau.hat = predict(test.forest,X= test_data[,important.var_cf], estimate.variance = T)
CATE_causalForest = tau.hat$predictions


#Predictor 1
group1 <- quantile(train_data[, Top2predictors[1]])
train_data$groupX1M <- ifelse(train_data[,Top2predictors[1]] >= group1[4], 4, 
                             ifelse(train_data[,Top2predictors[1]]>=group1[3] & train_data[,Top2predictors[1]]<group1[4], 3,
                                    ifelse(train_data[,Top2predictors[1]]>=group1[2] & train_data[,Top2predictors[1]]<group1[3], 2,
                                           1)))
train_data$groupX1M <- factor(train_data$groupX1M)
boxplot(CATE_causalForest ~ train_data$groupX1M, xlab = paste(Top2predictors[1], "Group"), ylab = "CATE")

#Predictor 2
group2 <- quantile(train_data[, Top2predictors[2]])
train_data$group2 <- ifelse(train_data[,Top2predictors[2]] >= group2[4], 4, 
                             ifelse(train_data[,Top2predictors[2]]>=group2[3] & train_data[,Top2predictors[2]]<group2[4], 3,
                                    ifelse(train_data[,Top2predictors[2]]>=group2[2] & train_data[,Top2predictors[2]]<group2[3], 2,
                                           1)))
train_data$group2 <- factor(train_data$group2)
boxplot(CATE_causalForest ~ train_data$group2, xlab = paste(Top2predictors[2], "Group"), ylab = "CATE")


```

# Comparisons across Methods
```{r  q2 supplementary, total, echo = T, include = T, message= F, warning = F, error = F}

labels = c("BART", "Causal Forests")

# correlation matrix

cor(cate_m, CATE_causalForest)

# qqplot 

cate_comp <- qqplot(cate_m, CATE_causalForest)

# boxplot 

cate_com_box <- boxplot(cate_m, CATE_causalForest, names = labels, ylab = "CATE")
  
```

# Method Comparisons and Conclusion

Insert BART conclusion

Insert GenericML conclusion

Causal Forest (CF) did not fit very well with the data set. Because when using the training data set, CF with the best tunning parameters detected heterogeneity, but it failed to detect heterogeneity by using the testing data set. When plotting the top two important predictors with CATE, we found no difference between groups.


Comparison

In summary,
