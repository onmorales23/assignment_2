---
title: "Data Analysis 2"
output: 
  pdf_document
author: Ziying Li, Matt Capaldi, Yujuan Gao, Olivia Morales
date: \today
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=T, include=T, message=F, warning=F, error=F}

## libraries
libs <- c("tidyverse", "haven", "bibtex", "psych", "knitr", "pastecs", "kableExtra","survey", "cobalt", "randomForest", "ipred","rpart", "baguette", "parsnip", "SimDesign", "bartCause", "lme4", "grf", "GenericML", "car", "bartMachine", "gtools")

sapply(libs, require, character.only = TRUE)

covariateNames <- c(
    "X1RTHETK1", 
    "X1MTHETK1",
    "X1TCHAPP", 
    "X1TCHCON",
    "X1TCHPER", 
    "X1TCHEXT",
    "X1TCHINT",
    "X1ATTNFS",
    "X1INBCNT",
    "X12MOMAR",
    "X1NUMSIB", 
    "P1OLDMOM",
    "P1CHLDBK",
    "P2DISTHM",
    "P1NUMPLA",
    "T2PARIN",
    "X12PAR1ED_I",
    "X12PAR2ED_I",
    "X2INCCAT_I",
    "X1PAR1EMP",
    "S2LUNCH",
    "X2KRCETH",
    "S2NGHBOR",
    "S2OUTSID",
    "S2USDABR",
    "S2PUBSOC",
    "X1LOCALE",
    "S1_ID",
    "W1_2P0PSU",
    "prop.missing",
    "X_CHSEX_R", "X_RACETH_R", "P1HSCALE")

covariateNamesBART <- c(
    "X1RTHETK1", 
    "X1MTHETK1",
    "X1TCHAPP", 
    "X1TCHCON",
    "X1TCHPER", 
    "X1TCHEXT",
    "X1TCHINT",
    "X1ATTNFS",
    "X1INBCNT",
    "X12MOMAR",
    "X1NUMSIB", 
    "P1OLDMOM",
    "P1CHLDBK",
    "P2DISTHM",
    "P1NUMPLA",
    "T2PARIN",
    "X12PAR1ED_I",
    "X12PAR2ED_I",
    "X2INCCAT_I",
    "X1PAR1EMP",
    "S2LUNCH",
    "X2KRCETH",
    "S2NGHBOR",
    "S2OUTSID",
    "S2USDABR",
    "S2PUBSOC",
    "X1LOCALE",
    "X_CHSEX_R", "X_RACETH_R", "P1HSCALE")

## directory path (assignment_2 as current working directory)

data_dir <- file.path(".", "data")

## loading data
load(file.path(data_dir, "chapter_10_data_cleaned_and_imputed.Rdata"))

#standardized continuous predictors
for (var in covariateNames) {
  if (class(data[,var])!="factor") { data[,var] = (data[,var]-mean(data[,var]))/sd(data[,var]) } }

```

In this paper, we explore heterogeneity of treatment using three different methods; CausalBART, GenericML, and CausalForest. We will explore the heterogeneity (Questions 1, 3, and 4) separately for each model before concluding and comparing CATES (Question 2) at the conclusion of the paper.

```{r pre q PSM model, echo = T, include = T, message=F, warning = F, error = F}


psFormula <- paste(covariateNames, collapse="+")
psFormula <- formula(paste("treated~", psFormula, sep=""))
print(psFormula) 
    
```

# BART

```{r q_1,BART, echo = T, include = T, message= F, warning = F, error = F}

# credit to Matt, esp for the covs matrices!
train <- data %>% 
  sample_frac(size = 0.5)
test <- anti_join(data, train)

matrix_covsBART <- as.matrix(train %>% select(covariateNamesBART) %>%
                           mutate(across(.fns = as.numeric))) 

matrix_covsBART_2 <- as.matrix(test %>% select(covariateNamesBART) %>%
                           mutate(across(.fns = as.numeric))) 

# estimating conditional average treatment effects (CATEs) using BART

bart <- bartc(response = train$X2MTHETK1,
             treatment = as.numeric(as.character(train$treated)),
             confounders = matrix_covsBART,
             method.rsp = "bart", 
             method.trt = "glm",
             keepTrees = TRUE,
             estimand = "ate")

cate <- predict(bart,
               newdata = matrix_covsBART_2,
               type = "icate")

cate_m <- apply(cate, 2, mean) 


library(bartMachine)

bart_machine <- bartMachine(X=as.data.frame(matrix_covsBART), 
                            y=cate_m,
                            serialize = T, 
                            mem_cache_for_speed = F)


var.importance <- investigate_var_importance(bart_machine, 
                                            num_replicates_for_avg = 25)

#select variables whose importance is greater than the median
important.vars = names(var.importance$avg_var_props)[
  var.importance$avg_var_props > median(var.importance$avg_var_props)]

test <- test %>% mutate(across(.fns = as.numeric))
test$treated <- test$treated - 1

bart2 <- bartc(response = test$X2MTHETK1,
              treatment = test$treated,
              confounders = data.frame(test[,important.vars]),
              method.rsp = "bart",
              method.trt = "glm",
              estimand = "ate",
              keepTrees = T) 

cate2 <- predict(bart2,
                newdata = data.frame(test[,important.vars]),
                type = "icate")

test$cate2 <- apply(cate2, 2, mean)

model <- paste(important.vars, collapse="+")
model <- paste(c("cate2",model), collapse="~")
model.cate <- lm(model,data = test)
summary(model.cate)

```



```{r q_2,BART, echo = T, include = T, message= F, warning = F, error = F}

# correlation matrix

# cor(bart2, cate2, method = c("pearson", "kendall", "spearman"))

# qqplot 

# plot1 <- gglot(sample = bart2, data = chapter_10_data_cleaned_and_imputed.Rdata, color=cyl)+theme_bw()


```

# Two most important CATE predictors (BART), plot CATE/predictor relationship

```{r q_4,BART, echo = T, include = T, message= F, warning = F, error = F}

## top 2 predictors
top.pred <- c("X1TCHINT", "X1NUMSIB")

qpredvar_tch <- quantcut(test$X1TCHINT, na.rm = TRUE)
qpredvar_num <-  quantcut(test$X1NUMSIB, na.rm = TRUE)

## plot relationship

bplot1 <- ggplot(test, aes(x = qpredvar_tch, y = cate2)) +
  geom_boxplot()

bplot2 <- ggplot(test, aes(x = qpredvar_num, y = cate2)) +
  geom_boxplot()

```

\newpage

# GenericML

### Q1)
Initially, we set up the hyper-parameters for GenericML. We decided to use the learners ranger (with 300 trees, for a balance of efficiency and effectiveness), and lasso with the default settings as a comparison.

As GenericML is set up to be used on experimental data and therefore does not accept propensity scores outside of 0.05 and 0.95, propensity scores were calculated and then rounded at the extreme ends to avoid this issue.

The other primary hyperparameter we were concerned with was the num_splits argument in the GenericML function (how many times the data is split for recalculating), which was set to 10. This again was  chosen for a balance efficiency and effectiveness.

```{r q_1 genML, echo = T, include = T, message = F, warning = F, error = F}

# Setup for GML
learners <- c("mlr3::lrn('ranger', num.trees = 300)", "lasso")
matrix_covs <- as.matrix(data %>% select(all_of(covariateNames)) %>%
                           mutate(across(.fns = as.numeric))) 
X1 <- setup_X1(funs_Z = c("B", "S"))
vcov <- setup_vcov(estimator = "vcovHC")

# Estimate ps scores (with 0.05/0.95 adjustment to work with GenML)
library(parsnip)
ps_rf <- rand_forest(mode = "classification",
               engine = "ranger",
               trees = 1000) %>%
  fit(psFormula,
      data = data)
data$ps_rf <- predict(ps_rf,
                      new_data = data,
                      type = "prob")[,2]
data$ps_rf <- data$ps_rf$.pred_1 ## remove the $column.name
data <- data %>%
  mutate(ps_rf = ifelse(ps_rf >= 0.95, 0.94, ps_rf), #Rounding to avoid error Dr. L
         ps_rf = ifelse(ps_rf <= 0.05, 0.06, ps_rf))

# Run initial GenML
genML <- GenericML(
  Z = matrix_covs, #covariates
  D = as.numeric(as.character(data$treated)), #treatment
  Y = as.numeric(data$X2MTHETK1), #outcome
  learners_GenericML = learners,  # learners specified above
  learner_propensity_score = as.numeric(data$ps_rf), #as.numeric(data$ps)  #ps
  num_splits = 10,                        # number splits of the data
  quantile_cutoffs = c(0.2, 0.4, 0.6, 0.8), # grouping for CATEs
  significance_level = 0.05,                # significance level
  X1_BLP = X1, X1_GATES = X1,               # regression setup
  vcov_BLP = vcov, vcov_GATES = vcov,       # covariance setup
  parallel = F, #num_cores = 6L, # parallelization
  seed = 20220621)                         # RNG seed
```

As the below printouts show, the best ranger performed more effectively than lasso, and therefore, the results calculated with ranger become the default for the rest of the analysis
```{r}
get_best(genML)
## ranger is best, becomes the default for all future GenML functions
```

Unlike when initially calculating heterogeneity on attendance, GenML does not find evidence of significant heterogeneity when looking at spring math scores. For the purpose of the assignment however, we continue analyzing as if there was heterogeneity to be explored.
```{r}
get_BLP(genML)
## (no longer) significant indicating treatment heterogeneity
```

```{r}
get_GATES(genML)
```

The below code predicts CATEs with all covariates for use in Q2 at the end of the paper.
```{r}
# Predict CATEs with full dataset for plotting/comparison
genML_Q2 <- proxy_CATE(Z = matrix_covs,
                    D = as.numeric(as.character(data$treated)),
                    Y = as.numeric(data$X2MTHETK1),
                    A_set = sample(1:12684, size = 12684/2), #obs sample half
                    learner = "mlr3::lrn('ranger', num.trees = 300)")
data$GenML_CATEa <- genML_Q2$estimates$CATE

```

### Q3)
The next stage for GenericML took a workaround to best imitate the variable importance or BartMachine functions of the other two methods. Using heterogeneity_CLAN() we were able to get the p-values for each variable in terms of covariates in terms of their influence on heterogeneity.

```{r}
genML_het <- heterogeneity_CLAN(genML)

genML_sig <- as.data.frame(genML_het$p_values) %>%
  pivot_longer(cols = everything()) %>%
  arrange(value)

# Selecting variables above median significance
genML_imps <- genML_sig %>%
  filter(value < median(value)) %>%
  select(name) %>%
  as.list()
genML_imps <- genML_imps[["name"]]
```

Then, a new covariate matrix was created with only variables whose p-value was less than the median of all. As there is no directly comparable function to variable importance in causal forests and bart machine for causal bart, this was our best approximation of a simialr concept.

We then recalculated CATEs with only these reduced covariates.
```{r}
# Create reduced covariates matrix
matrix_covsGML2 <- as.matrix(data %>% select(all_of(genML_imps)) %>%
                           mutate(across(.fns = as.numeric))) 

# Calculate CATES using reduced covaraites
genML_Q3 <- proxy_CATE(Z = matrix_covsGML2,
                    D = as.numeric(as.character(data$treated)),
                    Y = as.numeric(data$X2MTHETK1),
                    A_set = sample(1:12684, size = 12684/2), #obs sample half
                    learner = "mlr3::lrn('ranger', num.trees = 300)")
data$GenML_CATEr <- genML_Q3$estimates$CATE
```

The resulting CATEs are summarized below. These results indicate that there is some variation, however, as noted above the omnibus test failed to find significant heterogeneity, so cannot assume this variation is anything other than noise. The correlation between the reduced and all variable CATE is 0.49 indicating a moderate relationship as demonstrated by the plot provided below as well
```{r}
summary(data$GenML_CATEr)

cor(data$GenML_CATEa, data$GenML_CATEr)
plot(data$GenML_CATEa, data$GenML_CATEr,
     main = "Comparing CATEs from GenML",
     ylab = "CATE with imp vars only",
     xlab = "CATE with all vars")
```

### Q4)

Lastly, we draw back on the calculation of individual predictors of treatment and pull out the 2 with the lowest p-values, X1ATTNFS (focus scale) and X1TCHAPP (teacher approaches to learning). Below are the plots of these variable against the CATE.
```{r}
genML_sig %>% head(n = 2)
# Show the 2 lowest p values of het, i.e. most significant predictors of het

a <- ggplot(data) +
  geom_point(aes(x = X1ATTNFS,
                 y = GenML_CATEa)) +
  labs(y = "CATE",
       x = "Focus Scale")


b <- ggplot(data) +
  geom_point(aes(x = X1TCHAPP,
                 y = GenML_CATEa)) +
  labs(y = "CATE",
       x = "Teacher Approach")

library(patchwork)
a + b

```

Even though these variables were identified as the mostly likely contributors to treatment heterogeneity, the 2 plots show quite there is no clear relationship to be seen between them and the CATE, supporting the finding that GenericML suggests there is no treatment heterogeneity.

\newpage

# Causal Forests

```{r q_1 CF, echo = T, include = T, message = F, warning = F, error = F}

data2 <- data

#fit logistic regression model for propensity score estimation ignoring clustering
ps.model0 <- glm(psFormula, data=data2, family=binomial)

#obtain propensity scores that ignore clustering
data2$ps <- fitted(ps.model0)

#the grf package only takes numeric covariates
#So convert those factor variables to be the numeric class
for (i in 1:length(covariateNames)) {
  if(class(data2[,covariateNames[i]])=="factor"){
    data2[, covariateNames[i]] <- as.numeric(as.character(data2[,covariateNames[i]]))
  }
}

#Step 1: Split data into training data set and testing data set
#In this case, we split it to be 50/50
set.seed(123)
train_index <- sample(1:nrow(data2), nrow(data2)/2)
train_index <- train_index[order(train_index)]

train_data <- data2[train_index,]
test_data <- data2[-train_index,]

#Step 2: model fit, using causal forest
#Tuning mtry and min.node.size parameters by setting tune.parameters
train.forest = causal_forest(X=train_data[,covariateNames],
                              Y = train_data$X2MTHETK1, num.trees = 5000,
                              W = as.numeric(as.character(train_data$treated)),
                              W.hat = train_data$ps,
                              tune.parameters = c("mtry", "min.node.size"),
                              seed = 0)

#The best tunning parameters of mtry and min.node.size were shown below, which indicated a better performance than the default setting
train.forest[["tuning.output"]]



```

# Figures Comparing Different CF fits

```{r q_2 CF, echo = T, include = T, message = F, warning = F, error = F}
#Obtain estimates of the conditional average treatment effect (CATE)
#with standard errors
tau.hat = predict(train.forest,X= test_data[,covariateNames], estimate.variance = T)
CATE_causalForest = tau.hat$predictions


# Causal Forests
#1. correlation matrix
#causal forest only output the best tunning parameters' model fit outcomes
#To answer Q2, I run one more model fit with mtry = 4 and min.node.size = 50
train.forest2 = causal_forest(X=train_data[,covariateNames],
                              Y = train_data$X2MTHETK1, num.trees = 5000,
                              W = as.numeric(as.character(train_data$treated)),
                              W.hat = train_data$ps,
                              mtry = 4, min.node.size = 50,
                              seed = 0)
tau.hat2 = predict(train.forest2,X= test_data[,covariateNames], estimate.variance = T)
CATE2_causalForest = tau.hat2$predictions

cor(CATE_causalForest, CATE2_causalForest)

#2.box plot
boxplot(CATE_causalForest, CATE2_causalForest)

#3.QQ plot
qqplot(CATE_causalForest, CATE2_causalForest)

#The correlation between the two tunning methods was strong. But the distributions of CATEs between these two tunning methods were slightly different (see box plot), which caused a nonlinear QQ plot.
```

# Determine Best Linear Projection of CATE/Variable Importance (Causal Forests)

```{r q_3, echo = T, include = T, message = F, warning = F, error = F}

#Causal Forests
#Step 1: Subset important variables
importance_cf = variable_importance(train.forest)
rownames(importance_cf) = names(train_data[,covariateNames])

#select variables above the median of importance of the aggregated importances
#across imputed datasets
important.var_cf = rownames(importance_cf)[importance_cf>median(importance_cf)]

#Step 2
#run test forest with the best hyperparameters
test.forest = causal_forest(X = test_data[,important.var_cf],
                            Y = test_data$X2MTHETK1,
                            W = as.numeric(as.character(test_data$treated)),
                            W.hat = test_data$ps,
                            mtry = 22, num.trees=5000,
                            min.node.size = 1, seed = 0)

#Step 3: Estimate the best linear projection of a conditional average treatment effect using a causal forest
predictors = test_data[,important.var_cf]

CATE.prediction = best_linear_projection(test.forest, A=predictors)
CATE.prediction

#The results showed no significant differences in each covariate (p > 0.05).
#check the herterogeneity
test_calibration(test.forest)
#The outcomes showed that the coefficient of the mean forest prediction was 1 
 #which indicated the mean forest prediction was correct. 
#Also, the results indicated no heterogeneity been detected.


```

# Two most important CATE predictors (Causal Forests), plot CATE/predictor relationship


```{r q_4, echo = T, include = T, message = F, warning = F, error = F}

#The most two predictors are
Top2predictors <- c("X1TCHEXT", "P1OLDMOM")

#Obtain estimates of the conditional average treatment effect (CATE)
#with standard errors
tau.hat = predict(test.forest,X= test_data[,important.var_cf], estimate.variance = T)
CATE_causalForest = tau.hat$predictions


#Predictor 1
group1 <- quantile(train_data[, Top2predictors[1]])
train_data$groupX1M <- ifelse(train_data[,Top2predictors[1]] >= group1[4], 4, 
                             ifelse(train_data[,Top2predictors[1]]>=group1[3] & train_data[,Top2predictors[1]]<group1[4], 3,
                                    ifelse(train_data[,Top2predictors[1]]>=group1[2] & train_data[,Top2predictors[1]]<group1[3], 2,
                                           1)))
train_data$groupX1M <- factor(train_data$groupX1M)
boxplot(CATE_causalForest ~ train_data$groupX1M, xlab = paste(Top2predictors[1], "Group"), ylab = "CATE")

#Predictor 2
group2 <- quantile(train_data[, Top2predictors[2]])
train_data$group2 <- ifelse(train_data[,Top2predictors[2]] >= group2[4], 4, 
                             ifelse(train_data[,Top2predictors[2]]>=group2[3] & train_data[,Top2predictors[2]]<group2[4], 3,
                                    ifelse(train_data[,Top2predictors[2]]>=group2[2] & train_data[,Top2predictors[2]]<group2[3], 2,
                                           1)))
train_data$group2 <- factor(train_data$group2)
boxplot(CATE_causalForest ~ train_data$group2, xlab = paste(Top2predictors[2], "Group"), ylab = "CATE")

#Both two plots for two important predictors did not show an obvious different CATEs among groups.

```

\newpage

# Comparisons across Methods (Q2)
```{r  q2 supplementary, total, echo = T, include = T, message= F, warning = F, error = F}

labels = c("BART", "Causal Forests", "GenericML")

# correlation matrix

cor(cate_m, CATE_causalForest, data$GenML_CATEa)

# qqplot 

cate_comp <- qqplot(cate_m, CATE_causalForest, data$GenML_CATEa)

# boxplot 

cate_com_box <- boxplot(cate_m, CATE_causalForest, data$GenML_CATEa,
                        names = labels, ylab = "CATE")
  
```

# Method Comparisons and Conclusion

Insert BART conclusion

GenericML failed to find any evidence of treatment heterogeneity on the math score outcome variable. The p-value of the omnibus test for heterogeneity was 0.58, so not even close to marginal significance. The CATEs calculated with all vars and only important vars were loosely similar to each other. The two most important variables still showed no real evidence of treatment heterogeneity.

Causal Forest (CF) did not fit very well with the data set. Because when using the training data set, CF with the best tunning parameters detected heterogeneity, but it failed to detect heterogeneity by using the testing data set. When plotting the top two important predictors with CATE, we found no difference between groups.

In summary, this paper attempted to explore treatment effect heterogeneity with Causal Forests, GenericML, and Causal BART. Unfortunately, there was no significant heterogeneity to be found in terms of math score outcome. All three methods came to similar conclusions.

```{r Code Junkyard, eval=FALSE}
